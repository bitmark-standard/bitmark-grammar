[.assignment]
[.chapter:1][#Transferleistung 1 Informatik]
**1.1	Einführung**
Eine HF-Ausbildung und dazu gehört auch dieser NDS, hat als primäres Ziel die Handlungskompetenz zum Inhalt. Es genügt also nicht, sich einfach Wissen anzueignen, man muss es auch anwenden können. Dieser Vorgang wird mit einer Transferleistung überprüft.
Transferleistungen haben somit immer einen Bezug zum Praxisumfeld des Studierenden. Sie wenden dabei ihr theoretisches Wissen an einem konkreten Fall an ihrem Arbeitsplatz an oder befassen sich, wie teilweise in diesem Fall,mit Fachartikeln die in der Praxis von Bedeutung sind.
Sie können die nachfolgenden Aufgaben 1, 2, 4 + 5 auch in Gruppen lösen, geben Sie aber an, wer in dieser Gruppe mitgearbeitet hat. Die Aufgabe 3 ist in jedem Fall individuell und bezogen auf das eigen Unternehmen zu lösen.

[.assignment:bitmark++]
**1.2	Bewertung**
Diese Arbeit 
*	zählt 70% zu ihrer Semesternote
*	Der Aufwand wird mit 3–4 Arbeitsstunden veranschlagt.
*	ist zeitlimitiert und muss bis spätestens 10.03.2015 eingereicht sein

[.assignment:bitmark--]
**1.3	Vorbereitung**
Erstellen Sie ein Textdokument (Word, Write etc.) und lösen Sie die nachfolgenden Aufgaben. An-schliessend erzeugen Sie eine PDF-Datei, nennen sie 
**Transferleistung1_Name_Vorname**
und senden Sie diese an mich unter __tfahrni@fah.ch__

**Formale Vorgaben**

__Schrift__
Für den Fliesstext ist eine Schriftgrösse von 11pt bei einem Zeilenabstand von 1.5 zu wählen. Referenzschriftart ist Arial (Typ Helvetica).

__Formate, Seitenränder__
*	Papierformat: A4
*	Ausdruck: einseitig
*	Paginierung: Beginnt erst mit dem Hauptteil
*	Zeilenabstand: 1.5-fach
Die Seitenränder werden folgendermassen festgelegt:
*	oben 	3 cm
*	unten 	2 cm 
*	links 	3 cm
*	rechts 	2 cm 
Empfohlen wird die Darstellung in Blocksatz. Dabei ist auf ausreichende Silbentrennung zu achten, um ein ausgewogenes Schriftbild zu erhalten.	

[.essay&article:bitmark++]
[%2	Aufgabe 1]
[!Lesen Sie den Artikel «Virtualisierung – aber sicher» und beantworten Sie die die dazu gestellten Fragen.]
Antwortform: kurze Sätze, max. 1 A-4 Seite
Zeitaufwand: 30 min.

&&&
**2.1	Virtualisierung – aber sicher**
__Autor: Marco Kündig für Swiss IT Magazine, Ausgabe 08/2009__
VIELE UNTERNEHMEN HALTEN ZENTRALE GESCHÄFTSANWENDUNGEN AUS SICHERHEITSGRÜNDEN VON VIRTUALISIERTEN UMGEBUNGEN FERN. NEUE TECHNOLOGIEN SOLLEN DIES NUN ÄNDERN.
Für Konsolidierung und Virtualisierung geben die Unternehmen im IT-Bereich am meisten aus. Zu diesem Schluss kommt Goldmann Sachs in ihrem CIO-Survey 2008. Der Trend ist eindeutig. Gemäss des Industry Analyst Reports  waren 2008 noch fünf Prozent der gesamten Arbeitsbelastung der Server virtualisiert, diese Zahl wird sich in den nächsten fünf Jahren auf 50 bis 60 Prozent erhöhen. Auch vor der Schweizer IT- und Telekommunikationsbranche macht die Virtualisierung nicht halt. Der Grund dafür ist einfach, denn mit Virtualisierungstechnologien lassen sich bestehende Hardware Ressourcen effizienter nutzen und damit nicht nur die Kosten für Hardware, sondern auch für Energie senken.

**2.2	Virtuelle Switches erhöhen Sicherheitsrisiken**
Häufig wird dabei aber vernachlässigt, dass die Server-Virtualisierung auch eine Reihe neuer Herausforderungen für die Server- und Netzwerk-Administratoren mit sich bringt. So wird mit der Virtualisierung der Rand des Netzwerkes automatisch in den Server hineinverschoben, denn jeder installierte Hypervisor  enthält einen oder mehrere virtuelle Switches. Diese Ausdehnung multipliziert nicht nur die Anzahl der Switches, die verwaltet werden müssen, sondern verschiebt die Kontrolle des Netzwerkes vom Netzwerk-Administrator in den Verantwortungsbereich der Server-Administratoren. Damit ist der Server-Administrator auf einen Schlag zuständig für das Netzwerk-Design, die Konfiguration, den VLAN-Aufbau , die Sicherheit der Anwendungen, das Monitoring und die Behebung von kurzfris-tigen Problemen. Diese Entwicklung birgt grosse Risiken. Dem Server-Administrator ist unter Umständen nicht einmal bewusst, dass virtuelle Switches in seiner Umgebung vorhanden sind, bis ein Problem auftritt. Indem all diese zusätzlichen Verantwortlichkeiten aus dem Gesichtsfeld des Netzwerk Administrators verschwinden, wächst die Gefahr von unabsichtlichen Fehlern bei der Konfiguration, und es drohen Sicherheitslöcher. Ausserdem können Probleme in der virtualisierten Server-Umgebung langsamer behoben werden. 
Des weiteren bringt eine virtualisierte Umgebung neue Sicherheitsherausforderungen mit sich. Im Vergleich zu Hardware-Switches verfügen Software-Switches nur über limitierte Sicherheitsfunktionen. Ein Grossteil der Verantwortung liegt bei einer Person - beim Server-Administrator. Damit wächst auch die Gefahr, dass eine einzelne Person die Sicherheitsbestimmungen und regulatorischen Anforderungen umgehen kann. Dieses Risiko veranlasst heute viele Unternehmen, die Art der virtualisierten Anwendungen zu limitieren oder virtualisierte Anwendungen zu isolieren. Gleichzeitig opfern die Firmen so Servereffizienz zugunsten operationeller Sicherheit. 

**2.3	Virtuell den Funktionsumfang eines physischen Switches**
Damit liegt in vielen Unternehmen das wahre Potenzial der Virtualisierung brach und wird nur ungenügend genutzt. Die Lösung und der Wunsch vieler Datacenter-Administratoren  ist folglich ein virtueller Switch mit den Sicherheitsfunktionen und den Administrationsmöglichkeiten eines physischen Switches. Im Zentrum dieser neuen Technologien steht die Möglichkeit, koordiniert Netzwerk- und Sicherheits-Services in Echtzeit zu konfigurieren, die durch den gesamten Lebenszyklus der virtuellen Maschine (VM) persistent sind. Das Managementmodell soll dabei VM-zentriert bleiben, sodass die Server-Administratoren ihre bestehenden Tools und operationellen Prozesse nutzen können, wenn sie neue Anwendungen bereitstellen.
Mit den neuen Virtualisierungs-Technologien lassen sich gleich mehrere Fliegen mit einer Klappe schlagen. Sie bieten Netzwerk- wie auch Server-Administratoren die Instrumente, um operationelle wie auch politische  Grenzen im Rechenzentrum zu überbrücken, wenn Server virtualisiert werden. Nahtlos eingebettet in die Virtualisierungsumgebungen vereinfachen sie das IT-Management sowohl auf physikalischer wie auch auf virtualisierter Ebene. Durch die direkte Implementierung eines verteilten virtuellen Switches in Virtualisierungsumgebungen werden die Optionen für Management und Konfiguration deutlich erweitert und verbessert. Erstmals können Administratoren richtlinienbasierte Netzwerk-Services, wie man sie von Netzwerk Hardware-Switches kennt, auch auf jede virtuelle Maschine anwenden. Virtuelle Server lassen sich bei laufendem Betrieb flexibel auf der physikalischen Serverinfrastruktur hin- und her verschieben. Die Geräte können ausserdem unterbrechungsfrei gewartet und der Workload der Serverfarm optimal ausbalanciert werden. Somit steigen Verfügbarkeit, Performance und Skalierbarkeit. 
Eine virtuelle Maschine kann also bezüglich Sicherheit, Troubleshooting und Logging gleich behandelt werden wie eine physische Maschine. So lassen sich auch eine Reihe von Sicherheitsbedenken entkräften, die bisher viele Unternehmen davon abgehalten haben, eine breite Palette an geschäftskritischen Anwendungen auf virtualisierten Servern zu betreiben. Der Virtualisierung als Instrument für die Unternehmen, um ihre Ressourcen effizienter einzusetzen sowie Strom und Platz zu sparen, eröffnen sich damit weitere und ungemein grössere Möglichkeiten.

Wer /was ist die «Industry Analyst Reports»? Antworten Sie in 2–3 Sätzen oder Stichworten.
[?Berichte über das Marktgeschehen, die von darauf spezialisierten
Unternehmen erbracht werden. Die Reports zeigen auf was auf dem Markt
passiert und wohin eine Entwicklung voraussichtlich gehen wird.]
Was ist ein Hypervisor?
[?Ein Hypervisor ist eine Virtualisierungssoftware, die eine Umgebung für
virtuelle Maschinen schafft. Die wichtigsten zwei sind Vmware vom
gleichnamigen Hersteller und Hyper-V von Microsoft]
Geben Sie an, wie sich ein LAN und ein VLAN unterscheiden? (2–3 Sätzen genügen)
[?Für den Benutzer gibt es keinen Unterschied (transparent), aber für den
Administrator. Bspw. Können auf zwei physischen Servern mehrere
virtuelle Server und Workstation laufen, die untereinander mit virtuellen
Switch verbunden sind. Virtuelle Geräte können physisch nicht ausfallen,
aber logisch, das heisst das Abbild des «Gerätes» kann zerstört werden.]
Wer hat (normalerweise) den grösseren Verantwortungsbereich: Netzwerk- oder Server-Administrator?
[?In der Regel der Netzwerkadministrator. Er ist für den Verbund, die
Sicherheit und Performance der IT-Umgebung verantwortlich.]
Nehmen Sie zur Aussage dieses Satzes Stellung. Was ist für Sie wichtiger?(2–3 Sätzen genügen)
Was ist ein Datacenter-Administrator?
[?Leiter eines Datacenters in diesem Kontext der Leiter der gesamten
Virtuellen IT-Umgebung. Er ist weniger für den operationellen Betrieb
zuständig, sondern für Konzeption der IT-Umgebung und Umsetzung von
Anforderungen.]
Was könnte mit politischen Grenzen gemeint sein? Nehmen Sie mit 3–4 Sätzen Stellung dazu.
[?Firmeninterne Abgrenzungen. Ressourcen werden an Abteilungen fix
zugeteilt, das ist mit virtuellen Netzwerken nicht unbedingt nötig]
Geben Sie in 3–4 Sätzen an, warum die drei Punkte in diesem Satz zutreffend sind.
[?Die Verfügbarkeit steigt, da Geräte jederzeit virtuell erzeugt werden
können
Die Performance steigt, da nicht verkabelt werden muss und viele
Abläufe (z.B. Datenverbindung/-verschiebung) nur logisch ablaufen
Die Skalierbarkeit ist gegeben, da man theoretisch beliebig viele Geräte
hinzufügen oder entfernen kann.]

||todo: bitmark like interview&article would be useful||

[.essay&article:bitmark++]
[%3	Aufgabe 2]
[!Lesen Sie den Artikel «Speicherprobleme Nachhaltig lösen» und beantworten Sie die die dazu gestellten Fragen.]
Antwortform: kurze Sätze, max. 1 A-4 Seite
Zeitaufwand: 30 min.

&&&
**3.1	Speicherprobleme Nachhaltig lösen**
Autor: Andreas Zeitler Publireportage  von Symantec, Computerworld Ausgabe 02/2010
IN DIESEM JAHR SIND VIELE IT-BUDGETS GEKÜRZT ODER EINGEFROREN WORDEN. COMPLIANCE ANFORDERUNGEN UND DATENMENGEN STEIGEN DAGEGEN STETIG. DIESE GEGENSÄTZLICHEN TENDENZEN ERFORDERN NEUE, KOSTENSPARENDE LÖSUNGSWEGE FÜR EFFIZIENTE RECHENZENTREN. DIE TRENDS: VIRTUALISIERUNG, KONSOLIDIERUNG, DATA LOSS PREVENTION UND DESASTER RECOVERY
Nicht nur die Datenmenge steigt in den Unternehmen laufend an, sondern auch der Wert der gespeicherten Informationen. Damit wächst der Druck auf die Verantwortlichen, sowohl Anwendungen als auch die zugehörigen Daten immer verfügbar zu haben. Die Mobilität der Anwender verschärft das Problem zusätzlich. Die Gründe für den stetig steigenden Speicherbedarf sind vielfältig. Insbesondere E-Mails mit ihren Anhängen sind dafür verantwortlich. Anhänge, aber auch sonstige Dateien, werden von den Benutzern oft mehrfach abgelegt.

**3.2	Trend: Virtualisierung**
Bei der Reduzierung des Hardware-Bedarfs spielen Software-Lösungen zur Virtualisierung von Servern und Storage-Kapazitäten  die Hauptrolle. Bei Software-Lösungen stehen eine Bestandsaufnahme der vorhandenen Kapazitäten und deren Leistungsfähigkeit am Anfang. Danach werden virtuelle Laufwerke eingerichtet, die über die physikalischen Grenzen der einzelnen Festplatten-Arrays in einem Speicher-Pool hinausgehen. 
Das hat den Vorteil, dass die Kapazitäten bedarfsgerecht aufgeteilt werden können: Wenn in Abteilung A kaum noch physikalischer Speicherplatz verfügbar ist, kann das virtuelle Volume  mit neuen physikalischen Platten aus dem Speicher-Pool oder mit ungenutzten Speicherkapazitäten des virtuellen Volumes aus Abteilung B aufgefüllt werden, wenn dieses Volume nur einen Bruchteil der verfügbaren Kapazität benötigt. So bleibt zumindest eine Anschaffung neuer Arrays für Abteilung A erst einmal unnötig.
Je nachdem, wie viel freie Kapazitäten in den verschiedenen Abteilungen oder an den verschiedenen Standorten des Unternehmens brachgelegen haben, kann sogar ein erheblicher Anteil der vorhandenen Festplatten-Arrays abgeschaltet werden. 
Weiterhin ist es möglich, virtuelle Volumes nach der Leistung der zugrunde liegenden Hardware zu klassifizieren: Volumes mit grosser Leistungsfähigkeit können Anwendungen mit einem grossen Datendurchsatz zugewiesen werden. Applikationen, die nur gelegentlich Daten anfordern, nutzen dagegen nur langsamere Laufwerke, die auch ohne grosse Leistungseinbussen zum Strom sparen in den Stand-by-Modus versetzt werden können.

**3.3	Trend: Konsolidierung**
Neben den Vorteilen der Virtualisierung werden auch Technologien, die Konsolidierung  in Rechenzentren unterstützen, zunehmend wichtiger. Lösungen,die eine umfassende Steuerung sowohl für physische als auch für virtuelle Umgebungen ermöglichen, erlauben einen proaktiven Umgang mit der wachsenden Komplexität in Rechenzentren und bieten eine Möglichkeit zur Umsetzung der oft immer anspruchsvolleren Service Level Agreements.
In einer virtualisierten Umgebung kann sich mehr als eine Applikation auf einem einzelnen physischen Speicher befinden. Das hat verschiedene Vorteile: Bessere Ausnutzung von Hardware-Ressourcen, vereinfachte Handhabung und Hardwareunabhängigkeit sind nur einige davon. Allerdings kann bei schlechtem Management dieses Konzept sehr komplex und risikoreich sein. Schliesslich sind sämtliche Applikationen, die auf einem einzelnen Server abgelegt sind, dann auch anfällig für Hardware-Fehler dieses Servers.
Bei grösseren Umgebungen ist es sinnvoll, auf automatisierte Konfigurationsmanagement-Lösungen zurückzugreifen. Indem diese die Konfiguration der IT-Systemlandschaft steuern, einen Überblick über Abhängigkeiten liefern und Veränderungen in Echtzeit verfolgen, liefern sie einen entscheidenden Beitrag zu einem ganzheitlichen Change Management. Eine einheitliche Integrationsplattform erlaubt die gemeinsame Administration von Bereichen wie Anmeldung, Einrichtungsprozess, Lizenzverwaltung, Berichtswesen, Arbeitsablauf, Benutzeroberfläche, CMDB-Integration (Configuration Ma-nagement Database) und Integration von Verzeichnissen. Im Unterschied zu Konsolen und Überwachungs-Tools, die lediglich eine Ansicht erlauben, ermöglicht es diese Plattform den IT-Administratoren, auftretende Probleme aktiv zu managen.

**3.4	Trend: Data Loss Prevention**
Unternehmen sind sich des Risikos bewusst, geschäftskritische und vertrauliche Daten verlieren zu können. Das ergab eine europaweit durchgeführte «Data Loss Prevention  (DLP)»- Studie von Symantec. Dennoch besteht eine Lücke zwischen Anspruch und Wirklichkeit —insbesondere, was die Sicherheit im Bereich mobiler Endgeräte betrifft: Obwohl die meisten Unternehmen über allgemeine Strategien zur Verhinderung von Datenverlusten verfügen, glaubt knapp die Hälfte der im Rahmen der Studie befragten Unternehmen, auf Massnahmen gegen den Verlust von Daten auf mobilen Geräten verzichten zu können. Zudem können die Mitarbeitenden häufig mobile Speichergeräte ohne Beschränkungen nutzen — ein Umstand, der vor dem Hintergrund aktueller und vergangener Datenskandale eine erhebliche Gefahr darstellen kann. Eine weitere Sorge ist Industriespionage: Ein Drittel schätzt die Gefahr für das Unternehmen als hoch oder sehr hoch ein. Nur knapp unter 40 Prozent verfügen jedoch über eine gesonderte Strategie zur Abwehr von Spionageattacken.
Über 50 Prozent der befragten Unternehmen fürchten den Verlust von Daten. Bei mehr als einem Drittel ist ein solcher Datenverlust bereits ein- oder mehrmals vorgekommen. Bei knapp einem Zehntel der Befragten ist dadurch ein Schaden von umgerechnet mehr als 15'000 Schweizer Franken entstanden. Drei Viertel der an der Studie beteiligten Unternehmen verfügen denn auch über eine Strategie zur Verhinderung von Datenverlusten. Die im Rahmen einer solchen Strategie am häufigsten verwendeten Instrumente sind Network Access Controls, Verschlüsselung und Device Control. Seltener kommen Data-Loss-Prevention- sowie Endpoint-Security -Management-Lösungen zum Einsatz.

**3.5	Trend: Disaster Recovery**
Da immer mehr Applikationen und Daten in einer virtuellen Umgebung verwaltet werden, suchen Unternehmen nach neuen Methoden und Technologien, die sich für die Datenwiederherstellung in virtuellen Umgebungen besonders eignen. Notfälle sind nicht so selten wie gemeinhin vermutet: Rund ein Drittel der im Rahmen des jährlichen Disaster Recovery (DR) Reports von Symantec befragten Unternehmen gab an, dass vorliegende Notfallpläne im vergangenen Jahr wenigstens teilweise zum Einsatz kamen.
Knapp die Hälfte der befragten Unternehmen testen ihre DR-Pläne jedoch nur einmal im Jahr oder seltener. Hauptgrund: Unternehmen befürchten Störungen in ihrem Geschäftsbetrieb. Immerhin ein Fünftel der Befragten geht davon aus, dass sich DR-Tests auf Verkäufe und Umsatz auswirken. Mit automatisierten Test-Tools lässt sich dies indes verhindern. Disaster-Recovery-Lösungen, die auf Virtualisierung zurückgreifen und sich nahtlos in virtuelle Umgebungen einfügen, zeigen einen gangbaren Weg, um die notwendigen Tests ohne Beeinträchtigung durchzuführen.
Immerhin ein Zehntel der Unternehmen in Europa gaben auch in der aktuellen Umfrage an, ihre DR-Pläne überhaupt nicht zu testen, obwohl die Folgen signifikant sind: Beinahe die Hälfte der befragten Unternehmen schätzt, dass sie eine komplette Woche benötigen würden, um wieder einen vollständig normalen Betrieb erreichen zu können.

**3.6	Trend: Grünes Rechenzentrum**
Der Kostendruck im Energiebereich, eine dünner werdende Personaldecke in der Administration und wachsende Leistungsanforderungen durch immer anspruchsvollere Dienstleistungsverpflichtungen haben einen enormen Bedarf geschaffen, Rechenzentren in Unternehmen weniger komplex und mit weniger Hardware aufzubauen. Die Virtualisierung von Servern und Storage-Kapazitäten ist ein Mittel, um dieses Ziel zu erreichen, weil dadurch eine erheblich verbesserte Auslastung der bereits vorhandenen Hardware-Ressourcen erreicht wird. Moderne Unternehmens-Software bietet diese Möglichkeiten und ist darüber hinaus hoch integriert und durch eine gemeinsame Oberfläche wesentlich einfacher zu administrieren. Allein durch die geschickte Wahl der Software-Lösung lassen sich also viele der Probleme bewältigen. Dazu gehört auch der Energiebedarf eines Rechenzentrums: Weniger physisch vorhandene Geräte benötigen weniger Strom und Kühlung.

Was ist eine Publireportage?
[?Publireportagen ist eine Werbeform, die die journalistische Form der
Reportage aufnimmt und versucht, ein Produkt oder eine Dienstleistung im
Kontext eines redaktionellen Beitrags zu verankern.]
Was ist mit Storage gemeint?
[?Speicherbereiche. In grösseren Umgebungen werden Daten nicht auf
Servern, sondern auf spezialisierten «Geräten» gelagert.]
Was ist in diesem Artikel mit «Volume» gemeint?
[?Ein Volum ist in der Regel eine Festplatte. In virtuellen Umgebungen (auch
in modernen Betriebssystemen) können mehrere Festplatten oder Teile
davon zu einem einzigen Volume vereinigt werden. Sie erscheinen dann als
eine einzige logische Einheit.]
Konsolidierung hat verschiedene Bedeutungen. Welche ist in diesem Artikel gemeint?
[?Das «gewachsene» IT-Umgebungen zusammengefasst, entflochten und
vereinfacht werden. Dabei werden Ressourcen besser ausgenutzt, die
Abläufe transparenter und die Ausfallsicherheit verbessert.]
Was ist damit «Data Loss Prevention» gemeint?
[?Massnahmen, die verhindern, dass Datenverluste auftreten. Das können
technische wie organisatorische Massnahmen sein.]
Was ist mit « Endpoint-Security» in diesem Zusammenhang gemeint?
[?Bei Endpoint-Security wird einem Gerät nur Zugriff gewährt, wenn es alle
Sicherheitsmassnahmen erfüllt (Virenschutz, Updates etc.). Das können 
Notebooks, Smartphones, oder Tablets sein. ES wird zentral verwaltet und
wirkt pro-aktiv.]
||todo: [?] showing||

[.essay&article:bitmark++]
[%4	Aufgabe 3]
[!Sie haben nun zwei Artikel zum Thema Virtualisierung bearbeitet. Zwischen Theorie und Praxis liegen oft Welten. Wie sieht es in ihrem Unternehmen aus.]
&&&
* Wurden bereits Virtualisierungen vorgenommen?
*	Wurden gute Erfahrungen gemacht?
*	Wurden die Erwartungen erfüllt?
*	Wird der eingeschlagene Weg weiterverfolgt? Wenn Nein, warum nicht?
*	Wenn noch keine Virtualisierungen vorgenommen wurden, sind solche geplant?
*	Welche Erwartungen sind an diese Planung gebunden?
*	Sind nebst Server- und Dienstvirtualisierungen auch Desktop/Workstation-Virtualisierungen vorgesehen?
Erkundigen sich in Ihrem Unternehmen nach Antworten über die oben erwähnten Fragen? Anonymisieren Sie dort, wo sicherheitsrelevante Bereiche betroffen sind, versuchen Sie aber ein reales Bild wiederzugeben. Fragen Sie im Zweifelsfall nach, ob die aufgeführten Daten als LL verwendet werden dürfen.
Sollten Sie wiedererwarten keinen Bericht über die Situation im eigenen Betrieb machen dürfen/können, suchen Sie sich im Internet eine Fallstudie (ähnlich dem Beispiel der Hero) und fassen Sie sie zusammen. Ziel ist es zu prüfen, wie gut ihre Kenntnisse in diesem Bereich sind. Wenn es ihnen wenig Mühe bereitet, die wesentlichen Punkte zu finden und diese in einem logischen Zusammenhang wiederzugeben, haben Sie das Ziel erreicht und können behaupten, das Thema verstanden zu haben.
Antwortform: kurze Sätze, max. 1 A-4 Seite
Zeitaufwand: 40 min.

[.interview:bitmark--]
[%5	Aufgabe 4]
[!Lesen Sie den nachfolgenden Artikel und lösen Sie dazu gestellten Aufgaben.]
Antwortform
Richtet sich nach den Fragen

**5.1	«Herausforderungen für den Datenschutz im Internet»**
Quelle: «Wirtschaftsinformatik» C. Laudon et al, © 2010 by Pearson Studium
Die Technik des Internets wirft neue Probleme in Bezug auf den Datenschutz auf. Daten, die über das Internet gesendet werden, passieren viele verschiedene Computersysteme, bevor sie ihr Ziel erreichen. Jedes dieser Systeme kann den Datenverkehr überwachen, abfangen und speichern.
Alle Onlineaktivitäten können aufgezeichnet werden. z. B. auf welche Newsgroups oder Dateien jemand zugegriffen, welche Webseiten er besucht und welche Produkte er sich angesehen oder bestellt hat. Zumeist erfolgt die Überwachung und Verfolgung von Website-Besuchen im Hintergrund ohne Wissen des Besuchers. Werkzeuge zur Überwachung im World Wide Web haben an Popularität gewonnen. Onlineshops können mit diesen feststellen, wer ihre Websites besucht und wie Angebote gezielter platziert werden können. Einige Unternehmen überwachen auch die Internetnutzung ihrer Mitarbeiter, um festzustellen, für welche Zwecke die Netzwerkressourcen genutzt werden.
Websites kennen die Identität von Besuchern. wenn diese sich freiwillig auf ihr registrieren, um ein Produkt bzw. eine Dienstleistung zu kaufen oder – z. B. Nachrichtenartikel – kostenlos zu erhalten. Um einen Besucher bei einem neuen Besuch einer Website wiederzuerkennen oder einen Besuch auch ohne Registrierung des Nutzers zu verfolgen, sind jedoch weitere Techniken wie z. B. **Cookies** notwendig. Cookies sind Informationen, die eine Website – zumeist als Datei – auf dem Computer eines Nutzers speichert. Die Website kann auf diese Informationen ohne Kenntnis des Nutzers zugreifen und somit den Webbrowser des Nutzers bei jedem Seitenaufruf sowie bei einem vollständig neuen Besuch der Website wiedererkennen. Hierzu wird im Cookie eine ID des Benutzers hinterlegt, mit der ein Webserver auf benutzerspezifische Datenbestände zugreifen kann. Damit kann auch in Erfahrung gebracht werden, was eine identifizierte Person während früherer Website-Besuche getan hat, und die Website entsprechend angepasst werden. Kauft ein Kunde zum Beispiel ein Buch auf der Website eines Buchhändlers und kehrt er später mit demselben Browser wieder auf die Site zurück, so wird er von der Site namentlich begrüsst und erhält auf Basis seiner letzten Einkäufe Buchempfehlungen. Das weiter oben vorgestellte Unternehmen DoubleClick verwendet Cookies, um das Verhalten von Besuchern auf Websites zu untersuchen.
Neben Cookies gibt es noch weniger auffällige und schwerer zu kontrollierende Techniken für die Überwachung im Internet. Zum einen sind dies **Web-Bugs** (auch Zählpixel, Web-Wanze oder clean GIF genannt). Ein Web-Bug ist ein in einer E-Mail oder einer Webseite eingebetteter Verweis auf eine kleine Grafik, die auf einem entfernten Server gespeichert ist. Bei einem Aufruf dieser Datei können die IP-Adresse und die ID des Benutzers sowie die angezeigte Seite bzw. E-Mail vom Server registriert werden. Entdecken lässt sich ein Web-Bug ohne eine entsprechende Funktion in der Software nur schwer, da er zumeist sehr klein und transparent ist. Zum anderen kann Spyware zur Überwachung eines Nutzers heimlich, z. B. Huckepack mit einer grösseren Anwendung, auf einem Computer installiert werden. Über die Spyware kann die Benutzung des Computers, z. B. die Tastaturanschläge oder der Aufruf von Webseiten, protokolliert und es können entsprechende Daten an einen Server gesendet werden.
In den USA ist es für Unternehmen zulässig, Daten aus Geschäftsvorgängen zu erfassen und diese Daten ohne die Einwilligung der betroffenen Person für Marketingzwecke zu nutzen. Betreiber von Websites begnügen sich grösstenteils damit, die Besucher durch Datenschutzerklärungen auf ihren Websites über die Verwendung der Daten zu informieren. Bei einigen Websites wurden diese Datenschutzerklärungen um die Möglichkeit erweitert, der Verwendung von personenbezogenen Daten für Marketingzwecken zu widersprechen. Dieses Vorgehen entspricht dem **Opt-out-Prinzip**, wonach die Erfassung und Verwendung personenbezogener Daten so lange zulässig ist, bis der Betroffene dies ausdrücklich untersagt. Diesem gegenüber steht das **Opt-in-Prinzip**. welches im europäischen Recht vorherrscht. Nach diesem Prinzip ist die Erfassung und Verwendung personenbezogener Daten so lange unzulässig, bis der Betroffene nicht ausdrücklich in die Erfassung und Verwendung seiner personenbezogenen Daten – explizit oder konkludent – einwilligt.
Die Onlinebranche zieht die Selbstkontrolle einer gesetzlichen Regelung zum Datenschutz vor. 1998 schlossen sich Mitglieder der Onlinebranche in der Online Privacy Alliance zusammen, um die Selbstkontrolle zu fördern und eine Reihe von Datenschutzerklärungen für ihre Mitglieder zu entwickeln. Diese Gruppe wirbt für die Verwendung von «Gütezeichen», z. B. TRUSTE, mit denen Websites zertifiziert werden, die bestimmten Datenschutzprinzipien entsprechen. Mit ihren Datenschutzerklärungen erlegen sich Unternehmen häufig jedoch nur geringe Einschränkungen auf. Aber auch ihre Kunden schützen sich häufig nicht in dem Masse, wie sie könnten. Obwohl die grosse Mehrheit der Onlinekunden angibt, sich um den Datenschutz im Internet zu sorgen, lesen weniger als die Hälfte von ihnen Datenschutzerklärungen auf Websites durch.

**5.2	Aufgaben/Fragen**
1.	Im Abschnitt «Herausforderungen für den Datenschutz im Internet» sind vier Begriffe fett ausgezeichnet. Definieren Sie sie je in 1–2 Sätzen.
===
===
===
===
===
2.	Der Text im obigen Abschnitt umschreibt die sogenannte «Informationelle Selbstbestimmung». Definieren Sie sie je in 2–3 Sätzen um was es sich dabei handelt.
===
===


[.interview:bitmark++]
[%6	Aufgabe 5]
[!Lesen Sie den nachfolgenden Artikel und lösen Sie dazu gestellten Aufgaben.]
Antwortform
Richtet sich nach den Fragen

**6.1	«Systemqualität: Datenqualität und Systemfehler»**
Quelle: «Wirtschaftsinformatik» C. Laudon et al, © 2010 by Pearson Studium
Die Debatte über Zurechenbarkeit und Haftung für unbeabsichtigte Folgen von Systemen wirft eine verwandte Frage auf: Wie lässt sich ein akzeptablen technisch brauchbarer Grad an Systemqualität bestimmen? Wann sollte der zuständige Manager sagen: «Mit den Tests aufhören! Wir haben alles Menschenmögliche getan, um diese Software zu perfektionieren. Jetzt kann sie ausgeliefert werden!»? Einzelne und Unternehmen müssen für vermeidbare und vorhersehbare Folgen, die sie hätten erkennen und korrigieren müssen, Verantwortung tragen. Der Graubereich besteht darin, dass manche Systemfehler nur zu sehr hohen Kosten gefunden und korrigiert werden können. Diese Kosten können so hoch sein, dass der Versuch wirtschaftlich untragbar ist, dieses Mass an Perfektion zu erreichen, weil sich dann niemand das Produkt leisten könnte. Der zur Korrektur sämtlicher Fehler erforderliche Zeit- und Kostenaufwand wäre so hoch, dass die Produkte wohl nie auf den Markt kämen (Rigdon, 1995). Was wäre, wenn das Produkt nicht auf den Markt käme? Würde eine wirtschaftliche Entwicklung behindert werden? Wofür trägt der Anbieter von Dienstleistungen Verantwortung? Sollte er ein Produkt, das nie ganz fehlerfrei sein kann, vom Markt nehmen, sollte er den Benutzer warnen oder das Risiko vernachlässigen (und den Käufer das Risiko tragen lassen)?
Die drei wichtigsten Quellen für schlechte Systemleistung sind Softwarefehler, Hardware - oder Ge-bäudeschäden, die durch Naturkatastrophen oder aus anderen Gründen verursacht werden, und qualitativ schlechte Eingabedaten. Es ist im Allgemeinen unmöglich, einen vollkommen mängelfreien Quellcode zu erstellen und es ist daher nur bedingt möglich, die Auswirkungen verbleibender Fehler abzuschätzen. Es gibt folglich technische Barrieren, die einer perfekten Software im Weg stehen. Nicht zuletzt müssen sich Benutzer der Möglichkeit von Systemausfällen aufgrund von Naturkatastrophen bewusst sein. Trotz dieser Herausforderung hat die Softwarebranche bislang noch keine Teststandards zur Produktion von Software mit akzeptablem, aber nicht perfektem Fehlerfreiheitsgrad entwickelt (Collins et al., 1994)
Obwohl die Presse häufig über Software- und Hardwarefehler berichtet, ist die Datenqualität die bei Weitem häufigste Ursache von Systemausfällen in Unternehmen. Wenige Unternehmen messen regelmässig die Qualität ihrer Daten, obwohl laut Studien verschiedener Organisationen die Datenfehlerrate zwischen 0,5 und 30% liegen kann (Redman, 1998).

**6.2	«Lebensqualität: Gefährdung durch Kriminalität und technischen Wandel»**
Neben den positiven steigen auch die negativen sozialen Folgen aus der Nutzung von IT potenziell mit deren zunehmender Leistung. Mobilität und Durchdringung der Alltagswelt. Viele dieser negativen Folgen sind nicht das Ergebnis eines Verstosses gegen Persönlichkeits- oder Eigentumsrechte. Sie entstehen häufig schleichend oder sogar ungewollt mit dem Einsatz von IT und können trotzdem Einzelne, die Gesellschaft und politische Institutionen schädigen. IT hat das Potenzial, gleichzeitig sowohl neuen Nutzen zu bringen als auch wertvolle Einrichtungen unserer Kultur und Gesellschaft zu schädigen. Wenn der Einsatz von IT nicht nur mit positiven, sondern auch negativen Folgen verbunden ist, wen sollte man für die negativen Folgen verantwortlich machen? Im Folgenden werden einige negative Folgen von Systemen skizziert Dabei wird jeweils die persönliche, soziale und politische Ebene betrachtet.
**Computerkriminalität:** Neue Technologien, und dazu zählt auch die Informationstechnologie, schaffen neue Möglichkeiten für strafbares Handeln. Zum einen entstehen neue wertvolle Ziele für kriminelle Handlungen. Dies gilt umso mehr, als dass Unternehmen, Regierungen, Schulen, Kirchen und sonstige Organisationen heute von Informationssystemen in hohem Masse abhängig und daher verwundbar sind. Zum anderen entstehen neue Werkzeuge, mit deren Hilfe anderen Schaden zugefügt werden kann. 
Unter Computerkriminalität werden Straftaten verstanden. die mithilfe eines Computers begangen werden oder sich gegen ein Computersystem richten. Die bedeutenden Straftatbestände der Computerkriminalität sind im Strafgesetzbuch (StGB) geregelt. Hierzu zählen insbesondere:
•	**Ausspähen von Daten**: Verschafft sich jemand unberechtigt Zugang zu fremden, besonders gesicherten digitalen Daten, so macht er sich des Ausspähens von Daten schuldig. Dies gilt sowohl für in einem Computersystem gespeicherte Daten als auch für Daten, die über ein Netz, z. B. dem Internet, übertragen werden. Dabei ist es unerheblich, welcher Art die Daten sind und welchen Zweck sie haben, sodass auch Programme unter diesen Datenbegriff fallen. Voraussetzung für eine Strafbarkeit der Handlung ist jedoch. dass die Daten so gesichert sind, dass weder aus Versehen ein Zugriff auf die Daten erfolgen noch dass ein technischer Laie die Sicherung überwinden kann.
•	**Computerbetrug**: Wenn sich jemand einen Vormögensvorteil dadurch verschafft, dass er einen Computer täuscht, so begeht er einen Computerbetrug. Dieser Straftatbestand gilt dann, wenn jemand das Ergebnis eines Computerprogramms bewusst so manipuliert, dass er davon profitiert. Dies ist zum Beispiel der Fall, wenn er das Ziel einer Überweisung durch die Manipulation eines Bankrechners so umlenkt, dass der Überweisungsbetrag seinem Konto gutgeschrieben wird. Ein Computerbetrug kann insbesondere dadurch erfolgen. dass ein Programm selbst manipuliert wird – z. B. durch einen Mitarbeiter der Entwicklungsabteilung –, dass fehlerhafte Daten eingegeben werden – die z. B. zu einem Pufferüberlauf führen – oder dass Daten eingegeben werden, die von der Person nicht genützt werden dürfen — z. B. eine fremde PIN / TAN-Kombination.
•	**Fälschung beweiserheblicher Daten**: Eine Fälschung beweiserheblicher Daten entspricht in etwa der Urkundenfälschung ausserhalb von Informationssystemen. Zu den beweiserheblichen Daten gehören insbesondere solche digitalen Dokumente, die eine geschäftliche Tätigkeit dokumentieren. Dies können z. B. digital gespeicherte Verträge oder per E-Mail übermittelte Angebote sein. Somit werden auch über das Internet geschlossene Verträge von diesem Straftatbestand erfasst. Würde man also z. B. den Kaufpreis für ein online gekauftes Gut in der zugehörigen Bestätigungsmail nachträglich ändern, könnte man sich strafbar machen.
•	**Datenveränderung**: Werden fremde Daten bewusst beschädigt, gelöscht, unterdrückt oder so manipuliert, dass sie nicht wie gewohnt verwendet werden können, so ist der Straftatbestand der Datenveränderung erfüllt. Sie kann als eine Art digitale Sachbeschädigung angesehen werden. Dabei kommt es nicht darauf an, dass die Daten einen besonderen Wert oder eine besondere Funktion haben, auch müssen sie nicht besonders gesichert sein. Damit kann es z. B. strafbar sein, einen sich selbst verbreitenden Virus oder Wurm in einen fremden Computer einzuschleusen, selbst dann, wenn dieser keine schädliche Wirkung entfalten kann.
•	**Computersabotage**: Der wachsenden Bedeutung, die IT für das Funktionieren von Unternehmen hat, trägt der Tatbestand der Computersabotage Rechnung. Er ist erfüllt, wenn jemand vorsätzlich ein Computersystem stört, das für einen fremden Betrieb, ein fremdes Unternehmen oder eine Behörde von wesentlicher Bedeutung ist. Eine solche Sabotage kann sowohl physisch – z. B. durch das Zerschneiden eines Stromkabels – als auch digital als eine Datenveränderung – z. B. durch das Einschleusen eines Virus – erfolgen. Voraussetzung ist jedoch,dass zentrale Unternehmens- bzw. Behördenabläufe von diesem Computersystem abhängig sind. Das Zerstören eines privaten PCs ist damit keine Computersabotage im Sinne des Gesetzes.

**6.3	Aufgaben/Fragen**
3.	Welche sind die drei wichtigsten Quellen für schlechte Systemleistung?
===
[@shortAnswer]
===
[@shortAnswer]
===
[@shortAnswer]
===
4.	Das einfache unbefugte Eindringen in geschützte Computersysteme erfüllt zunächst keinen dieser Straftatbestände. Geben Sie die Gründe wieder, warum es trotzdem zu einer Anklage kommen kann.
===

||todo:
-weird ex names 
-which bits should be used for the texts and multiple questions
-title and ex names combo
-caps lock paragraphs||
